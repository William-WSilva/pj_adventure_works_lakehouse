<img src="./assets/img/lake_house_azure.png" width="600"/>

## üìå Vis√£o Geral do Projeto

Este projeto tem como objetivo construir uma pipeline de dados escal√°vel e automatizada para uma empresa do setor de varejo, utilizando servi√ßos gerenciados do Azure e adotando a arquitetura moderna Lakehouse (Bronze ‚Üí Silver ‚Üí Gold). A solu√ß√£o entrega dados prontos para an√°lise em dashboards do Power BI, apoiando decis√µes estrat√©gicas e operacionais do neg√≥cio.

<img src="assets/img/Arquitetura.png" width="900"/>


### üéØ Objetivos

- Ingest√£o automatizada de dados a partir de banco de dados relacional (Azure SQL Database)
- Armazenamento estruturado em camadas no Azure Data Lake com Delta Parquet
- Processamento e transforma√ß√£o de dados com Azure Databricks
- Orquestra√ß√£o via Azure Data Factory
- Governan√ßa e versionamento via GitHub com integra√ß√£o CI/CD
- Cria√ß√£o de tabelas agregadas para visualiza√ß√£o anal√≠tica com Power BI

### üß∞ Tecnologias Utilizadas


- ‚úÖ **Azure Data Factory (ADF)**  orquestra√ß√£o e ingest√£o de dados
- ‚úÖ **Azure Data Lake Storage Gen2 (ADLS)**  armazenamento em camadas
- ‚úÖ **Azure Databricks**  processamento distribu√≠do e notebooks de transforma√ß√£o
- ‚úÖ **Azure Key Vault**  gerenciamento seguro de segredos
- ‚úÖ **GitHub**  versionamento e CI/CD dos notebooks
- ‚úÖ **Power BI**  visualiza√ß√£o de dados e relat√≥rios anal√≠ticos

<br>

<img src="assets/img/tecnologias_ultilizadas.png" width="600"/>

<br>

## üóÇÔ∏è Arquitetura e Fluxo de Dados

A arquitetura adotada segue o padr√£o medalh√£o Lakehouse, separando o armazenamento e processamento de dados em tr√™s camadas: Bronze (Raw), Silver (Curated) e Gold (Business). A orquestra√ß√£o da ingest√£o √© feita com Azure Data Factory e o processamento com **Azure Databricks** processamento de dados com spark.

### üîÅ Ingest√£o (ADF ‚Üí Bronze)

A ingest√£o √© realizada de forma automatizada e parametrizada a partir de uma planilha de controle. Nessa planilha, defino quais tabelas est√£o ativas para ingest√£o, permitindo ativar ou desativar facilmente tabelas no pipeline conforme a necessidade.

 
<img src="assets/img/Controle_ingestao.png" width="400"/>


<br>

###  Data Factory: Copy Activity e ForEach

1. Criei uma atividade de c√≥pia full (toda tabela) que recebe par√¢metros e extrai dados diretamente do Azure SQL Database para arquivos .parquet na camada Bronze, utilizando infer√™ncia autom√°tica de schema.

2. Implementei uma consulta SQL que filtra apenas as tabelas marcadas como ativas para ingest√£o na tabela de controle.

3. Configurei um loop ForEach que itera sobre cada tabela ativa e, para cada uma, executa a atividade de c√≥pia correspondente, passando os par√¢metros de forma din√¢mica.

<br>

<img src="assets/img/Data_Factory2.png" width="600"/>

<br>

## üìÅ Estrutura de pastas do Projeto e Integra√ß√£o com GitHub

O projeto segue uma estrutura modular de diret√≥rios, separando claramente cada etapa da pipeline (ingest√£o, transforma√ß√£o e an√°lise). Al√©m disso, foram criadas pastas auxiliares para configura√ß√µes, testes e gera√ß√£o de dados sint√©ticos.

### üìÇ Organiza√ß√£o de Diret√≥rios

```bash
src/
‚îú‚îÄ‚îÄ 001_bronze_ingestion/         # Notebooks de ingest√£o de dados brutos (ADF ‚Üí ADLS)
‚îú‚îÄ‚îÄ 002_silver_transformation/    # Notebooks para tratamento e limpeza dos dados
‚îú‚îÄ‚îÄ 003_gold_transformation/      # Notebooks para aplica√ß√£o de regras de neg√≥cio e KPIs
‚îú‚îÄ‚îÄ configs/                      # Configura√ß√µes globais e notebooks de setup
‚îú‚îÄ‚îÄ data_analysis/                # Scripts e notebooks explorat√≥rios para an√°lises
‚îú‚îÄ‚îÄ data_generation/              # Scripts para gerar dados de exemplo ou mockados
‚îú‚îÄ‚îÄ tests/                        # Testes unit√°rios e de valida√ß√£o de dados
‚îú‚îÄ‚îÄ utils/                        # Fun√ß√µes reutiliz√°veis e helpers
‚îî‚îÄ‚îÄ README.md                     # Documenta√ß√£o do projeto
```

### üîó Integra√ß√£o com GitHub

O reposit√≥rio do projeto foi integrado ao **Databricks** via GitHub, permitindo versionamento de todo o c√≥digo (notebooks) e colabora√ß√£o eficiente entre os desenvolvedores. 
Cada notebook √© mantido no formato de arquivo .py com as marca√ß√µes do Databricks ``(%md, %sql, %python)``, garantindo compatibilidade com o GitHub e possibilitando a automa√ß√£o CI/CD.


#### ‚úÖ Exemplo de uso via UI do Databricks:

1. V√° em **Repos** ‚Üí **Git** ‚Üí **GitHub**
2. Selecione o reposit√≥rio e branch desejado
3. Trabalhe com controle de vers√£o diretamente do ambiente do Databricks

<img src="assets/img/Integracao_GitHub2.png" width="400"/>

<br>

## üñ•Ô∏è Cluster Utilizado

O projeto foi desenvolvido utilizando um cluster Databricks com a seguinte configura√ß√£o:

| Configura√ß√£o           | Valor                                |
|------------------------|-------------------------------       |
| **Databricks Runtime** | 15.4 LTS (Spark 3.5.0, Scala 2.12)   |
| **Node Type**          | Standard_DS3_v2                      |
| **Mem√≥ria / CPU**      | 14 GB RAM / 4 Cores                  |
| **Tipo de Cluster**    | Single Node                          |

<br>

## üõ°Ô∏è Montagem do Data Lake com Secret Scope

Antes de iniciar o processamento, montei o sistema de arquivos do **Azure Data Lake Storage Gen2 (ADLS)** no Databricks, utilizando autentica√ß√£o OAuth 2.0 via uma App Registration no Azure AD. Armazenei as credenciais sens√≠veis no **Azure Key Vault** e as disponibilizei no Databricks por meio de um Secret Scope chamado `ws_adb_secret_scope`.

O procedimento de montagem est√° implementado no notebook  `src/configs/mount_data_lake.py`

#### üîê Exemplo de leitura dos segredos e montagem do Data Lake

```python
# Obter segredos do Key Vault via dbutils
client_id     = dbutils.secrets.get('ws_adb_secret_scope', 'client-id')
tenant_id     = dbutils.secrets.get('ws_adb_secret_scope', 'tenant-id')
client_secret = dbutils.secrets.get('ws_adb_secret_scope', 'client-secret')

# Exemplo de implementa√ß√£o de montagem do container no Data Lake
    if  montar_mount_point:
        dbutils.fs.mount(
            source=f"abfss://{container_name}@{storage_acount_name}.dfs.core.windows.net/",
            mount_point=mount_point,
            extra_configs=configs)
    else:
        print(f"O container j√° est√° montado: {mount_point}")
```

<br>

## üß± Cria√ß√£o dos Databases no Databricks

Depois de montar o Data Lake, criei tr√™s databases no metastore do Databricks para organizar os dados de acordo com as camadas Bronze, Silver e Gold.

O notebook `create_databases` localizado na pasta `src/configs/` cont√©m os comandos SQL respons√°veis por essa etapa.

#### üì¶ Estrutura dos databases

- `adventure_works_bronze` ‚Üí Dados brutos ingeridos diretamente da fonte
- `adventure_works_silver` ‚Üí Dados tratados e normalizados
- `adventure_works_gold` ‚Üí Dados agregados prontos para an√°lise

#### Cria√ß√£o dos databases

```sql
-- Seleciona o metastore
USE CATALOG hive_metastore;

-- Cria database Bronze
CREATE DATABASE IF NOT EXISTS adventure_works_bronze
LOCATION '/mnt/wsadlsadwprd/adw/001_bronze/adventure_works_bronze';

-- Cria database Silver
CREATE DATABASE IF NOT EXISTS adventure_works_silver
LOCATION '/mnt/wsadlsadwprd/adw/001_silver/adventure_works_silver';

-- Cria database Gold
CREATE DATABASE IF NOT EXISTS adventure_works_gold
LOCATION '/mnt/wsadlsadwprd/adw/001_gold/adventure_works_gold';
```

<br>

## üîÑÔ∏è Processamento para a Camada Bronze

Ap√≥s a ingest√£o dos dados via Azure Data Factory, os arquivos `.parquet` ficam dispon√≠veis na camada **Bronze** do Data Lake. A partir disso, criei um notebook no Databricks para automatizar o registro desses dados como tabelas no metastore do Databricks.

O c√≥digo respons√°vel por essa etapa est√° no notebook: `src/001_bronze_ingestion/bronze_processing.py`

#### üîÅ Estrat√©gia de Processamento

- Desenvolvi a fun√ß√£o `generate_ingestion_dict()`, que percorre os diret√≥rios do Data Lake e gera dinamicamente um dicion√°rio com todos os arquivos a serem processados.
- Para cada entrada nesse dicion√°rio, a fun√ß√£o `ingest_bronze_and_save()` registra a tabela correspondente no database adventure_works_bronze, utilizando o formato `parquet`.
- Implementei a ``execu√ß√£o em paralelo`` utilizando ``ThreadPoolExecutor`` para otimizar o tempo de processamento, mesmo em um cluster com recursos limitados.

#### üìå Exemplo representativo

```python
# Caminho e database alvo
folders_path = '/mnt/wsadlsadwprd/adw/001_bronze'
database_name = 'hive_metastore.adventure_works_bronze'

# Gera√ß√£o din√¢mica do dicion√°rio com todos arquivos do container bronze
tables_ingestion_dic = generate_ingestion_dict(folders_path)

# Processamento paralelo com at√© 10 threads
with ThreadPoolExecutor(max_workers=10) as executor:
    for table_info in tables_ingestion_dic.values():
        executor.submit(process_table, table_info)
```

- ‚úÖ Com essa abordagem, todas as tabelas da Bronze s√£o catalogadas automaticamente no metastore, com ingest√£o paralela e tratamento de erros em tempo de execu√ß√£o.

<br>

## üîÑÔ∏è Transforma√ß√µes na Camada Silver

Desenvolvi notebooks de transforma√ß√£o individuais para cada tabela da camada Silver, aplicando regras espec√≠ficas de qualidade, limpeza e conformidade de schema em cada conjunto de dados. Essa separa√ß√£o por tabela garante modularidade, rastreabilidade e facilidade de manuten√ß√£o.

Esses notebooks de transforma√ß√£o Silver est√£o organizados na pasta: `src/002_silver_transformation/`

#### üîç Tabela: `person_emailaddress`

Notebook: `person_emailaddress.py`

- Realizei a substitui√ß√£o de valores nulos nas colunas ``rowguid`` (gerando um UUID) e ``ModifiedDate`` (usando o timestamp atual).
- Removi registros duplicados com base nas chaves ``BusinessEntityID`` e ``EmailAddressID``, utilizando uma janela ``ROW_NUMBER()`` ordenada pela data de modifica√ß√£o.
- Validei o schema dos dados por meio de um ``StructType`` expl√≠cito.
- Apliquei convers√µes de tipo (``cast``) para garantir os tipos de dados corretos em cada coluna.
- Implementei um upsert para refletir tamb√©m exclus√µes (``DELETE``), utilizando a fun√ß√£o ``_upsert_silver_table``.


üìå  Implementei todas as transforma√ß√µes acima em **PySpark**, reutilizando fun√ß√µes centralizadas no m√≥dulo ``common_functions`` para promover reuso e padroniza√ß√£o.



### ‚öôÔ∏è Orquestra√ß√£o da Camada Silver

Para orquestrar a execu√ß√£o dos notebooks da camada Silver de forma automatizada, criei um dicion√°rio que mapeia o nome de cada tabela ao caminho do seu notebook correspondente. Com esse dicion√°rio, controlo de forma flex√≠vel quais notebooks ser√£o executados.

#### üóÇÔ∏è Dicion√°rio de Execu√ß√£o

```python
dict_tables = {
    "person_address": {
        "active": 1,
        "notebook_path": "person_address"
    },
    "person_countryregion": {
        "active": 1,
        "notebook_path": "person_countryregion"
    },
    ...
    ...
}
```

Cada entrada no dicion√°rio representa uma tabela e aponta para o notebook de transforma√ß√£o correspondente (por exemplo: ``src/002_silver_transformation/{notebook_path}.py)``.

üöÄ Execu√ß√£o Paralela dos Notebooks

Para melhorar a performance, implementei a execu√ß√£o paralela dos notebooks ativos utilizando ``ThreadPoolExecutor``. Com isso, as transforma√ß√µes ocorrem de forma simult√¢nea, respeitando o n√∫mero de workers definido no cluster.

```python
# Submiss√£o dos notebooks ativos
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = {
        executor.submit(run_notebook, config): table
        for table, config in dict_tables.items()
        if config['active'] == 1
    }
```

Implementei a fun√ß√£o run_notebook() para encapsular a chamada dos notebooks e o tratamento de exce√ß√µes, garantindo maior resili√™ncia durante o processo.

‚úÖ Essa abordagem permite adicionar, remover ou pausar notebooks facilmente, alterando apenas o dicion√°rio ‚Äî sem necessidade de reescrever o pipeline.


<img src="assets/img/workflows_databricks.jpg" width="600"/>


<br>

## üîÑÔ∏è Camada Gold: Transforma√ß√µes Anal√≠ticas

A camada ``Gold`` consolida os dados tratados da Silver em tabelas dimensionais e factuais, prontas para an√°lise e visualiza√ß√£o. Essas tabelas seguem o modelo de Data Warehouse (esquema estrela) e representam entidades de neg√≥cio, como clientes, produtos, vendas e promo√ß√µes.

Os notebooks dessa camada est√£o organizados na pasta ``src/003_gold_transformation/``.

#### üîß Exemplo: `DimProduct`

Notebook: `DimProduct.py`

- Realizei jun√ß√µes entre tabelas de produtos, categorias, subcategorias e modelos.
- Removi duplicatas em cada dimens√£o utilizando uma janela ordenada por ModifiedDate.
- Gerei uma tabela dimensional de produtos com os principais atributos, pronta para an√°lise.
- Validei o schema e salvei os dados no formato Delta (tabela pronta para consulta).

#### üìà Exemplo: `FactInternetSales`

Notebook: `FactInternetSales.py`

- Une pedidos, clientes, ofertas, territ√≥rios e hist√≥rico de custo
- Aplica deduplica√ß√£o por IDs e filtra apenas pedidos online
- Calcula colunas derivadas como `LineTotal`, `DiscountAmount` e `TotalProductCost`
- Verifica integridade (ex: quantidade e pre√ßo maiores que zero)
- Valida o schema esperado e grava a tabela fato


### ‚öôÔ∏è Orquestra√ß√£o Paralela da Gold

Assim como na Silver, implementei a execu√ß√£o dos notebooks da Gold de forma paralela, utilizando um dicion√°rio para especificar os notebooks ativos e seus respectivos caminhos.

```python
dict_tables = {
    "DimProduct": {
        "active": 1,
        "notebook_path": "DimProduct"
    },
    ...
}
```

‚úÖ Com isso, a camada Gold entrega dados modelados e confi√°veis para an√°lises de performance, vendas, comportamento do cliente e outras vis√µes estrat√©gicas do neg√≥cio.


## üìò Aprendizados do Projeto

Ao longo deste projeto, eu consolidei e apliquei conhecimentos essenciais em engenharia de dados, destacando-se:

- **Arquitetura Lakehouse**: Estruturei as camadas Bronze, Silver e Gold, compreendendo na pr√°tica os benef√≠cios da separa√ß√£o entre ingest√£o bruta, tratamento e disponibiliza√ß√£o anal√≠tica dos dados.

- **Orquestra√ß√£o e paralelismo**: Utilizei paralelismo com `ThreadPoolExecutor` e orquestra√ß√£o com Azure Data Factory para melhorar a performance e escalabilidade do pipeline no Databricks.

- **Integra√ß√£o CI/CD com GitHub**: Implementei pipelines de CI/CD com ``versionamento`` dos notebooks (``.py``) via ``GitHub``, garantindo automa√ß√£o de deploys, rastreabilidade de mudan√ßas e colabora√ß√£o entre equipes.

- **Seguran√ßa com Secret Scopes**: Adotei o Azure Key Vault e Databricks Secret Scopes para proteger credenciais, promovendo boas pr√°ticas de seguran√ßa em ambientes de produ√ß√£o.

- **Organiza√ß√£o modular**: Estruturei os notebooks de forma clara e reutiliz√°vel por dom√≠nio, com fun√ß√µes comuns centralizadas em m√≥dulos auxiliares, facilitando a manuten√ß√£o e evolu√ß√£o do projeto.

- **Depura√ß√£o e performance**: Enfrentei e superei desafios relacionados a ``schemas``, performance e erros de execu√ß√£o, o que aprimorou minha capacidade de troubleshooting, valida√ß√£o de dados e otimiza√ß√£o de transforma√ß√µes.

Essas experi√™ncias refor√ßaram minha capacidade t√©cnica e pr√°tica na constru√ß√£o de pipelines robustos, seguros e escal√°veis com Databricks e Azure.


